{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ\n",
    "РОССИЙСКОЙ ФЕДЕРАЦИИ\n",
    "\n",
    "<p style=\"text-align: center;\">Федеральное государственное автономное\n",
    "образовательное учреждение высшего образования\n",
    "«Самарский национальный исследовательский университет\n",
    "имени академика С. П. Королева»\n",
    "(Самарский университет)</p>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align: center;\">Институт информатики и кибернетики\n",
    "    \n",
    "<p style=\"text-align: center;\">Факультет информатики\n",
    "    \n",
    "<p style=\"text-align: center;\">Кафедра программных систем\n",
    "    \n",
    " <br><br><br>   \n",
    "\n",
    "<p style=\"text-align: center;\">ОТЧЁТ\n",
    "\n",
    "<p style=\"text-align: center;\">по лабораторной работе № 1\n",
    "<p style=\"text-align: center;\">«Введение в модель MapReduce»\n",
    "<p style=\"text-align: center;\">по курсу «Интеллектуальный анализ и большие данные»\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align: right;\">Выполнил: Лазарев М.Ю.\n",
    "<p style=\"text-align: right;\">гр. 6132-020402D\n",
    "<p style=\"text-align: right;\">\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">Самара 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82OvPKEiEqjc"
   },
   "source": [
    "# Введение в модель MapReduce на Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQ2cvXLjICmI"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple # requires python 3.6+\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yjPHumVwEyEg"
   },
   "outputs": [],
   "source": [
    "def MAP(_, row:NamedTuple):\n",
    "  if (row.gender == 'female'):\n",
    "    yield (row.age, row)\n",
    "    \n",
    "def REDUCE(age:str, rows:Iterator[NamedTuple]):\n",
    "  sum = 0\n",
    "  count = 0\n",
    "  for row in rows:\n",
    "    sum += row.social_contacts\n",
    "    count += 1\n",
    "  if (count > 0):\n",
    "    yield (age, sum/count)\n",
    "  else:\n",
    "    yield (age, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBKMgpG_ilaZ"
   },
   "source": [
    "Модель элемента данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Rv-XIjhTJPx3"
   },
   "outputs": [],
   "source": [
    "class User(NamedTuple):\n",
    "  id: int\n",
    "  age: str\n",
    "  social_contacts: int\n",
    "  gender: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5KV0Ze2vQgu5"
   },
   "outputs": [],
   "source": [
    "input_collection = [\n",
    "    User(id=0, age=55, gender='male', social_contacts=20),\n",
    "    User(id=1, age=25, gender='female', social_contacts=240),\n",
    "    User(id=2, age=25, gender='female', social_contacts=500),\n",
    "    User(id=3, age=33, gender='female', social_contacts=800)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFeqzyZxZIFZ"
   },
   "source": [
    "Функция RECORDREADER моделирует чтение элементов с диска или по сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S5HR4E_GQoMJ"
   },
   "outputs": [],
   "source": [
    "def RECORDREADER():\n",
    "  return [(u.id, u) for u in input_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "NeEoWla-ROUy",
    "outputId": "94ca6e0e-4644-4282-acbf-1759d7ba2918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, User(id=0, age=55, social_contacts=20, gender='male')),\n",
       " (1, User(id=1, age=25, social_contacts=240, gender='female')),\n",
       " (2, User(id=2, age=25, social_contacts=500, gender='female')),\n",
       " (3, User(id=3, age=33, social_contacts=800, gender='female'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RECORDREADER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YB8orgPSZs8M"
   },
   "outputs": [],
   "source": [
    "def flatten(nested_iterable):\n",
    "  for iterable in nested_iterable:\n",
    "    for element in iterable:\n",
    "      yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "74oyvDLaRmd5",
    "outputId": "c6147702-7153-47c7-a574-d5fe6abe29a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, User(id=1, age=25, social_contacts=240, gender='female')),\n",
       " (25, User(id=2, age=25, social_contacts=500, gender='female')),\n",
       " (33, User(id=3, age=33, social_contacts=800, gender='female'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_output = flatten(map(lambda x: MAP(*x), RECORDREADER()))\n",
    "map_output = list(map_output) # materialize\n",
    "map_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, User(id=1, age=25, social_contacts=240, gender='female')),\n",
       " (25, User(id=2, age=25, social_contacts=500, gender='female')),\n",
       " (33, User(id=3, age=33, social_contacts=800, gender='female'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupbykey(iterable):\n",
    "  t = {}\n",
    "  for (k2, v2) in iterable:\n",
    "    t[k2] = t.get(k2, []) + [v2]\n",
    "  return t.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8ncYDJ3-VzDn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25,\n",
       "  [User(id=1, age=25, social_contacts=240, gender='female'),\n",
       "   User(id=2, age=25, social_contacts=500, gender='female')]),\n",
       " (33, [User(id=3, age=33, social_contacts=800, gender='female')])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_output = groupbykey(map_output)\n",
    "shuffle_output = list(shuffle_output)\n",
    "shuffle_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "cKzY_6COWOA2",
    "outputId": "e6791b12-e409-47e9-bcd4-e9f8ca8611bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 370.0), (33, 800.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_output = flatten(map(lambda x: REDUCE(*x), shuffle_output))\n",
    "reduce_output = list(reduce_output)\n",
    "reduce_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf6qhHEtd6bI"
   },
   "source": [
    "Все действия одним конвейером!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dZaQGYxCdpw5",
    "outputId": "3f5c6425-e5c5-49d2-b2cd-ce58a9acc33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 370.0), (33, 800.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flatten(map(lambda x: REDUCE(*x), groupbykey(flatten(map(lambda x: MAP(*x), RECORDREADER()))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq3EWRIpwSiJ"
   },
   "source": [
    "# **MapReduce**\n",
    "Выделим общую для всех пользователей часть системы в отдельную функцию высшего порядка. Это наиболее простая модель MapReduce, без учёта распределённого хранения данных. \n",
    "\n",
    "Пользователь для решения своей задачи реализует RECORDREADER, MAP, REDUCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V1PZeQMwwVjc"
   },
   "outputs": [],
   "source": [
    "def flatten(nested_iterable):\n",
    "  for iterable in nested_iterable:\n",
    "    for element in iterable:\n",
    "      yield element\n",
    "\n",
    "def groupbykey(iterable):\n",
    "  t = {}\n",
    "  for (k2, v2) in iterable:\n",
    "    t[k2] = t.get(k2, []) + [v2]\n",
    "  return t.items()\n",
    "\n",
    "def MapReduce(RECORDREADER, MAP, REDUCE):\n",
    "  return flatten(map(lambda x: REDUCE(*x), groupbykey(flatten(map(lambda x: MAP(*x), RECORDREADER())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFIVrimep678"
   },
   "source": [
    "## Спецификация MapReduce\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "f (k1, v1) -> (k2,v2)*\n",
    "g (k2, v2*) -> (k3,v3)*\n",
    " \n",
    "mapreduce ((k1,v1)*) -> (k3,v3)*\n",
    "groupby ((k2,v2)*) -> (k2,v2*)*\n",
    "flatten (e2**) -> e2*\n",
    " \n",
    "mapreduce .map(f).flatten.groupby(k2).map(g).flatten\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtTFyqke3KGe"
   },
   "source": [
    "# Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNhh5763w5Vn"
   },
   "source": [
    "## SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QkyurnvGxBGk",
    "outputId": "84761282-d2ba-435a-e8d7-a85150730e10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 370.0), (33, 800.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import NamedTuple # requires python 3.6+\n",
    "from typing import Iterator\n",
    "\n",
    "class User(NamedTuple):\n",
    "  id: int\n",
    "  age: str\n",
    "  social_contacts: int\n",
    "  gender: str\n",
    "    \n",
    "input_collection = [\n",
    "    User(id=0, age=55, gender='male', social_contacts=20),\n",
    "    User(id=1, age=25, gender='female', social_contacts=240),\n",
    "    User(id=2, age=25, gender='female', social_contacts=500),\n",
    "    User(id=3, age=33, gender='female', social_contacts=800)\n",
    "]\n",
    "\n",
    "def MAP(_, row:NamedTuple):\n",
    "  if (row.gender == 'female'):\n",
    "    yield (row.age, row)\n",
    "    \n",
    "def REDUCE(age:str, rows:Iterator[NamedTuple]):\n",
    "  sum = 0\n",
    "  count = 0\n",
    "  for row in rows:\n",
    "    sum += row.social_contacts\n",
    "    count += 1\n",
    "  if (count > 0):\n",
    "    yield (age, sum/count)\n",
    "  else:\n",
    "    yield (age, 0)\n",
    " \n",
    "def RECORDREADER():\n",
    "  return [(u.id, u) for u in input_collection]\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "output = list(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNKYIeerx0nY"
   },
   "source": [
    "## Matrix-Vector multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "rwcntRcCyi1V",
    "outputId": "606737ab-6b55-455c-931f-4fc45155f8a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, np.float64(1.9595716296562489)),\n",
       " (1, np.float64(1.9595716296562489)),\n",
       " (2, np.float64(1.9595716296562489)),\n",
       " (3, np.float64(1.9595716296562489)),\n",
       " (4, np.float64(1.9595716296562489))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import numpy as np\n",
    "\n",
    "mat = np.ones((5,4))\n",
    "vec = np.random.rand(4) # in-memory vector in all map tasks\n",
    "\n",
    "def MAP(coordinates:(int, int), value:int):\n",
    "  i, j = coordinates\n",
    "  yield (i, value*vec[j])\n",
    " \n",
    "def REDUCE(i:int, products:Iterator[NamedTuple]):\n",
    "  sum = 0\n",
    "  for p in products:\n",
    "    sum += p\n",
    "  yield (i, sum)\n",
    "\n",
    "def RECORDREADER():\n",
    "  for i in range(mat.shape[0]):\n",
    "    for j in range(mat.shape[1]):\n",
    "      yield ((i, j), mat[i,j])\n",
    "      \n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "output = list(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruZREYdi2o4O"
   },
   "source": [
    "## Inverted index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "vt9H9Alf3TYv",
    "outputId": "51aeffc9-e111-4607-bd84-cfcc7b56f238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', ['0', '1']),\n",
       " ('it', ['0', '1', '2']),\n",
       " ('is', ['0', '1', '2']),\n",
       " ('banana', ['2']),\n",
       " ('a', ['2'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "d1 = \"it is what it is\"\n",
    "d2 = \"what is it\"\n",
    "d3 = \"it is a banana\"\n",
    "documents = [d1, d2, d3]\n",
    "\n",
    "def RECORDREADER():\n",
    "  for (docid, document) in enumerate(documents):\n",
    "    yield (\"{}\".format(docid), document)\n",
    "      \n",
    "def MAP(docId:str, body:str):\n",
    "  for word in set(body.split(' ')):\n",
    "    yield (word, docId)\n",
    " \n",
    "def REDUCE(word:str, docIds:Iterator[str]):\n",
    "  yield (word, sorted(docIds))\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "output = list(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7az-6DA6qr2"
   },
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dN-nbtgG6uYG",
    "outputId": "24117576-7931-401d-a581-28e246b23453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 3), ('it', 9), ('is', 9), ('what', 5), ('a', 1), ('banana', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "d1 = \"\"\"\n",
    "it is what it is\n",
    "it is what it is\n",
    "it is what it is\"\"\"\n",
    "d2 = \"\"\"\n",
    "what is it\n",
    "what is it\"\"\"\n",
    "d3 = \"\"\"\n",
    "it is a banana\"\"\"\n",
    "documents = [d1, d2, d3]\n",
    "\n",
    "def RECORDREADER():\n",
    "  for (docid, document) in enumerate(documents):\n",
    "    for (lineid, line) in enumerate(document.split('\\n')):\n",
    "      yield (\"{}:{}\".format(docid,lineid), line)\n",
    "\n",
    "def MAP(docId:str, line:str):\n",
    "  for word in line.split(\" \"):  \n",
    "    yield (word, 1)\n",
    " \n",
    "def REDUCE(word:str, counts:Iterator[int]):\n",
    "  sum = 0\n",
    "  for c in counts:\n",
    "    sum += c\n",
    "  yield (word, sum)\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "output = list(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0:0', ''),\n",
       " ('0:1', 'it is what it is'),\n",
       " ('0:2', 'it is what it is'),\n",
       " ('0:3', 'it is what it is'),\n",
       " ('1:0', ''),\n",
       " ('1:1', 'what is it'),\n",
       " ('1:2', 'what is it'),\n",
       " ('2:0', ''),\n",
       " ('2:1', 'it is a banana')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(RECORDREADER())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-jRAcYCAkkk"
   },
   "source": [
    "# MapReduce Distributed\n",
    "\n",
    "Добавляется в модель фабрика RECORDREARER-ов --- INPUTFORMAT, функция распределения промежуточных результатов по партициям PARTITIONER, и функция COMBINER для частичной аггрегации промежуточных результатов до распределения по новым партициям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nw-b-xJsApgW"
   },
   "outputs": [],
   "source": [
    "def flatten(nested_iterable):\n",
    "  for iterable in nested_iterable:\n",
    "    for element in iterable:\n",
    "      yield element\n",
    "\n",
    "def groupbykey(iterable):\n",
    "  t = {}\n",
    "  for (k2, v2) in iterable:\n",
    "    t[k2] = t.get(k2, []) + [v2]\n",
    "  return t.items()\n",
    "      \n",
    "def groupbykey_distributed(map_partitions, PARTITIONER):\n",
    "  global reducers\n",
    "  partitions = [dict() for _ in range(reducers)]\n",
    "  for map_partition in map_partitions:\n",
    "    for (k2, v2) in map_partition:\n",
    "      p = partitions[PARTITIONER(k2)]\n",
    "      p[k2] = p.get(k2, []) + [v2]\n",
    "  return [(partition_id, sorted(partition.items(), key=lambda x: x[0])) for (partition_id, partition) in enumerate(partitions)]\n",
    " \n",
    "def PARTITIONER(obj):\n",
    "  global reducers\n",
    "  return hash(obj) % reducers\n",
    "  \n",
    "def MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, PARTITIONER=PARTITIONER, COMBINER=None):\n",
    "  map_partitions = map(lambda record_reader: flatten(map(lambda k1v1: MAP(*k1v1), record_reader)), INPUTFORMAT())\n",
    "  if COMBINER != None:\n",
    "    map_partitions = map(lambda map_partition: flatten(map(lambda k2v2: COMBINER(*k2v2), groupbykey(map_partition))), map_partitions)\n",
    "  reduce_partitions = groupbykey_distributed(map_partitions, PARTITIONER) # shuffle\n",
    "  reduce_outputs = map(lambda reduce_partition: (reduce_partition[0], flatten(map(lambda reduce_input_group: REDUCE(*reduce_input_group), reduce_partition[1]))), reduce_partitions)\n",
    "  \n",
    "  print(\"{} key-value pairs were sent over a network.\".format(sum([len(vs) for (k,vs) in flatten([partition for (partition_id, partition) in reduce_partitions])])))\n",
    "  return reduce_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxirlf3XqZxY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Спецификация MapReduce Distributed\n",
    "\n",
    "\n",
    "```\n",
    "f (k1, v1) -> (k2,v2)*\n",
    "g (k2, v2*) -> (k3,v3)*\n",
    " \n",
    "e1 (k1, v1)\n",
    "e2 (k2, v2)\n",
    "partition1 (k2, v2)*\n",
    "partition2 (k2, v2*)*\n",
    " \n",
    "flatmap (e1->e2*, e1*) -> partition1*\n",
    "groupby (partition1*) -> partition2*\n",
    "\n",
    "mapreduce ((k1,v1)*) -> (k3,v3)*\n",
    "mapreduce .flatmap(f).groupby(k2).flatmap(g)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWYw_CpbbY3C"
   },
   "source": [
    "## WordCount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "uR_zfGFkMZlp",
    "outputId": "c8d46167-473d-43b9-881a-2396991b3731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 key-value pairs were sent over a network.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, [('', 6), ('a', 2), ('is', 18), ('it', 18), ('what', 10)]),\n",
       " (1, [('banana', 2)])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import numpy as np\n",
    "\n",
    "d1 = \"\"\"\n",
    "it is what it is\n",
    "it is what it is\n",
    "it is what it is\"\"\"\n",
    "d2 = \"\"\"\n",
    "what is it\n",
    "what is it\"\"\"\n",
    "d3 = \"\"\"\n",
    "it is a banana\"\"\"\n",
    "documents = [d1, d2, d3, d1, d2, d3]\n",
    "\n",
    "maps = 3\n",
    "reducers = 2\n",
    "\n",
    "def INPUTFORMAT():\n",
    "  global maps\n",
    "  \n",
    "  def RECORDREADER(split):\n",
    "    for (docid, document) in enumerate(split):\n",
    "      for (lineid, line) in enumerate(document.split('\\n')):\n",
    "        yield (\"{}:{}\".format(docid,lineid), line)\n",
    "      \n",
    "  split_size =  int(np.ceil(len(documents)/maps))\n",
    "  for i in range(0, len(documents), split_size):\n",
    "    yield RECORDREADER(documents[i:i+split_size])\n",
    "\n",
    "def MAP(docId:str, line:str):\n",
    "  for word in line.split(\" \"):  \n",
    "    yield (word, 1)\n",
    " \n",
    "def REDUCE(word:str, counts:Iterator[int]):\n",
    "  sum = 0\n",
    "  for c in counts:\n",
    "    sum += c\n",
    "  yield (word, sum)\n",
    "  \n",
    "# try to set COMBINER=REDUCE and look at the number of values sent over the network \n",
    "partitioned_output = MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, COMBINER=None) \n",
    "partitioned_output = [(partition_id, list(partition)) for (partition_id, partition) in partitioned_output]\n",
    "partitioned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 key-value pairs were sent over a network.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, [('', 6), ('a', 2), ('is', 18), ('it', 18), ('what', 10)]),\n",
       " (1, [('banana', 2)])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_output = MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, COMBINER=REDUCE) \n",
    "partitioned_output = [(partition_id, list(partition)) for (partition_id, partition) in partitioned_output]\n",
    "partitioned_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCJGx8IQ87xS"
   },
   "source": [
    "## TeraSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "P2v8v1v_8_YR",
    "outputId": "e0987c25-9757-46cb-8e55-d5d2adfbee2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 key-value pairs were sent over a network.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [(None, np.float64(0.02368120145168562)),\n",
       "   (None, np.float64(0.06041786880809963)),\n",
       "   (None, np.float64(0.10419408512124773)),\n",
       "   (None, np.float64(0.12421364810380697)),\n",
       "   (None, np.float64(0.1308927544346007)),\n",
       "   (None, np.float64(0.16104313263937808)),\n",
       "   (None, np.float64(0.19571891579968925)),\n",
       "   (None, np.float64(0.20253802148856215)),\n",
       "   (None, np.float64(0.2298776958566402)),\n",
       "   (None, np.float64(0.22996041204377493)),\n",
       "   (None, np.float64(0.2362639459264605)),\n",
       "   (None, np.float64(0.2623838291138748)),\n",
       "   (None, np.float64(0.3224666015301897)),\n",
       "   (None, np.float64(0.34227884434083145)),\n",
       "   (None, np.float64(0.4307186839266227)),\n",
       "   (None, np.float64(0.4572915757635009)),\n",
       "   (None, np.float64(0.4805018232252508))]),\n",
       " (1,\n",
       "  [(None, np.float64(0.5176216961731316)),\n",
       "   (None, np.float64(0.6774263610600813)),\n",
       "   (None, np.float64(0.7183119362745176)),\n",
       "   (None, np.float64(0.738979597039996)),\n",
       "   (None, np.float64(0.7495654265725202)),\n",
       "   (None, np.float64(0.7543613684828773)),\n",
       "   (None, np.float64(0.7552881780219888)),\n",
       "   (None, np.float64(0.8069483372220856)),\n",
       "   (None, np.float64(0.8559175789060183)),\n",
       "   (None, np.float64(0.9246445870757583)),\n",
       "   (None, np.float64(0.9313225935452509)),\n",
       "   (None, np.float64(0.9709325950511123)),\n",
       "   (None, np.float64(0.9875704928113577))])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_values = np.random.rand(30)\n",
    "maps = 3\n",
    "reducers = 2\n",
    "min_value = 0.0\n",
    "max_value = 1.0\n",
    "\n",
    "def INPUTFORMAT():\n",
    "  global maps\n",
    "  \n",
    "  def RECORDREADER(split):\n",
    "    for value in split:\n",
    "        yield (value, None)\n",
    "      \n",
    "  split_size =  int(np.ceil(len(input_values)/maps))\n",
    "  for i in range(0, len(input_values), split_size):\n",
    "    yield RECORDREADER(input_values[i:i+split_size])\n",
    "    \n",
    "def MAP(value:int, _):\n",
    "  yield (value, None)\n",
    "  \n",
    "def PARTITIONER(key):\n",
    "  global reducers\n",
    "  global max_value\n",
    "  global min_value\n",
    "  bucket_size = (max_value-min_value)/reducers\n",
    "  bucket_id = 0\n",
    "  while((key>(bucket_id+1)*bucket_size) and ((bucket_id+1)*bucket_size<max_value)):\n",
    "    bucket_id += 1\n",
    "  return bucket_id\n",
    "\n",
    "def REDUCE(value:int, _):\n",
    "  yield (None,value)\n",
    "  \n",
    "partitioned_output = MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, COMBINER=None, PARTITIONER=PARTITIONER)\n",
    "partitioned_output = [(partition_id, list(partition)) for (partition_id, partition) in partitioned_output]\n",
    "partitioned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MQhoJaVZI93G",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m partitioned_output \u001b[38;5;241m=\u001b[39m \u001b[43mMapReduceDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUTFORMAT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mREDUCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOMBINER\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREDUCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPARTITIONER\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPARTITIONER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m partitioned_output \u001b[38;5;241m=\u001b[39m [(partition_id, \u001b[38;5;28mlist\u001b[39m(partition)) \u001b[38;5;28;01mfor\u001b[39;00m (partition_id, partition) \u001b[38;5;129;01min\u001b[39;00m partitioned_output]\n\u001b[0;32m      3\u001b[0m partitioned_output\n",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m, in \u001b[0;36mMapReduceDistributed\u001b[1;34m(INPUTFORMAT, MAP, REDUCE, PARTITIONER, COMBINER)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COMBINER \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m   map_partitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m map_partition: flatten(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m k2v2: COMBINER(\u001b[38;5;241m*\u001b[39mk2v2), groupbykey(map_partition))), map_partitions)\n\u001b[1;32m---> 29\u001b[0m reduce_partitions \u001b[38;5;241m=\u001b[39m \u001b[43mgroupbykey_distributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_partitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPARTITIONER\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shuffle\u001b[39;00m\n\u001b[0;32m     30\u001b[0m reduce_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m reduce_partition: (reduce_partition[\u001b[38;5;241m0\u001b[39m], flatten(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m reduce_input_group: REDUCE(\u001b[38;5;241m*\u001b[39mreduce_input_group), reduce_partition[\u001b[38;5;241m1\u001b[39m]))), reduce_partitions)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m key-value pairs were sent over a network.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(vs) \u001b[38;5;28;01mfor\u001b[39;00m (k,vs) \u001b[38;5;129;01min\u001b[39;00m flatten([partition \u001b[38;5;28;01mfor\u001b[39;00m (partition_id, partition) \u001b[38;5;129;01min\u001b[39;00m reduce_partitions])])))\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mgroupbykey_distributed\u001b[1;34m(map_partitions, PARTITIONER)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m map_partition \u001b[38;5;129;01min\u001b[39;00m map_partitions:\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m (k2, v2) \u001b[38;5;129;01min\u001b[39;00m map_partition:\n\u001b[1;32m---> 17\u001b[0m     p \u001b[38;5;241m=\u001b[39m partitions[\u001b[43mPARTITIONER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk2\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     18\u001b[0m     p[k2] \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mget(k2, []) \u001b[38;5;241m+\u001b[39m [v2]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(partition_id, \u001b[38;5;28msorted\u001b[39m(partition\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m (partition_id, partition) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(partitions)]\n",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m, in \u001b[0;36mPARTITIONER\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m     27\u001b[0m bucket_size \u001b[38;5;241m=\u001b[39m (max_value\u001b[38;5;241m-\u001b[39mmin_value)\u001b[38;5;241m/\u001b[39mreducers\n\u001b[0;32m     28\u001b[0m bucket_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m((\u001b[43mkey\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbucket_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbucket_size\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m ((bucket_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbucket_size\u001b[38;5;241m<\u001b[39mmax_value)):\n\u001b[0;32m     30\u001b[0m   bucket_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bucket_id\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "partitioned_output = MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, COMBINER=REDUCE, PARTITIONER=PARTITIONER)\n",
    "partitioned_output = [(partition_id, list(partition)) for (partition_id, partition) in partitioned_output]\n",
    "partitioned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Output: [(0, [(np.float64(0.07457520048751265), np.float64(0.07457520048751265)), (np.float64(0.1684072247838545), np.float64(0.1684072247838545)), (np.float64(0.21547690728665703), np.float64(0.21547690728665703)), (np.float64(0.23642423983868743), np.float64(0.23642423983868743)), (np.float64(0.23772022031997508), np.float64(0.23772022031997508)), (np.float64(0.24608489711452874), np.float64(0.24608489711452874)), (np.float64(0.29029160991882286), np.float64(0.29029160991882286)), (np.float64(0.3000751345653565), np.float64(0.3000751345653565)), (np.float64(0.33940805504525096), np.float64(0.33940805504525096)), (np.float64(0.4146635592989292), np.float64(0.4146635592989292)), (np.float64(0.4447449853872948), np.float64(0.4447449853872948)), (np.float64(0.44983898084183904), np.float64(0.44983898084183904)), (np.float64(0.4899961436221161), np.float64(0.4899961436221161))]), (1, [(np.float64(0.5086300571891559), np.float64(0.5086300571891559)), (np.float64(0.5220998785740798), np.float64(0.5220998785740798)), (np.float64(0.5472768671724506), np.float64(0.5472768671724506)), (np.float64(0.5952737028095999), np.float64(0.5952737028095999)), (np.float64(0.6362853592114097), np.float64(0.6362853592114097)), (np.float64(0.6566238913875233), np.float64(0.6566238913875233)), (np.float64(0.6679746934870643), np.float64(0.6679746934870643)), (np.float64(0.6742120386759742), np.float64(0.6742120386759742)), (np.float64(0.7361919927476597), np.float64(0.7361919927476597)), (np.float64(0.7662890930813602), np.float64(0.7662890930813602)), (np.float64(0.7678470680230576), np.float64(0.7678470680230576)), (np.float64(0.781253498008739), np.float64(0.781253498008739)), (np.float64(0.8382718240077447), np.float64(0.8382718240077447)), (np.float64(0.851468207402865), np.float64(0.851468207402865)), (np.float64(0.9011205345592984), np.float64(0.9011205345592984)), (np.float64(0.9026306657504153), np.float64(0.9026306657504153)), (np.float64(0.9327661172883652), np.float64(0.9327661172883652))])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_values = np.random.rand(30)\n",
    "maps = 3\n",
    "reducers = 2\n",
    "min_value = 0.0\n",
    "max_value = 1.0\n",
    "\n",
    "def INPUTFORMAT():\n",
    "    global maps\n",
    "    \n",
    "    def RECORDREADER(split):\n",
    "        for value in split:\n",
    "            yield (value, value)  # Теперь передаем сам value как key\n",
    "    split_size = int(np.ceil(len(input_values) / maps))\n",
    "    for i in range(0, len(input_values), split_size):\n",
    "        yield RECORDREADER(input_values[i:i + split_size])\n",
    "\n",
    "def MAP(value: float, key):\n",
    "    yield (key, value)  # Ключ равен значению\n",
    "\n",
    "def PARTITIONER(key):\n",
    "    global reducers, max_value, min_value\n",
    "    if key is None:\n",
    "        raise ValueError(\"Ошибка: key в PARTITIONER == None!\")  \n",
    "\n",
    "    bucket_size = (max_value - min_value) / reducers\n",
    "    bucket_id = int((key - min_value) / bucket_size)\n",
    "    bucket_id = min(bucket_id, reducers - 1) \n",
    "    return bucket_id\n",
    "\n",
    "def REDUCE(key, values):\n",
    "    for value in values:\n",
    "        yield (key, value) \n",
    "\n",
    "def groupbykey_distributed(map_partitions, PARTITIONER):\n",
    "    partitions = [{} for _ in range(reducers)]\n",
    "    for map_partition in map_partitions:\n",
    "        for (k2, v2) in map_partition:\n",
    "            p = partitions[PARTITIONER(k2)]\n",
    "            p[k2] = p.get(k2, []) + [v2]\n",
    "    return [(partition_id, sorted(partition.items(), key=lambda x: x[0])) for (partition_id, partition) in enumerate(partitions)]\n",
    "\n",
    "def MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, PARTITIONER, COMBINER=None):\n",
    "    record_readers = list(INPUTFORMAT())\n",
    "    map_outputs = map(lambda record_reader: flatten(map(lambda x: MAP(*x), record_reader)), record_readers)\n",
    "    map_partitions = list(map(lambda x: list(x), map_outputs))  # Преобразуем в список для отладки\n",
    "\n",
    "    if COMBINER is not None:\n",
    "        map_partitions = map(lambda map_partition: flatten(map(lambda k2v2: COMBINER(*k2v2), groupbykey(map_partition))), map_partitions)\n",
    "\n",
    "    reduce_partitions = groupbykey_distributed(map_partitions, PARTITIONER) \n",
    "    reduce_outputs = map(lambda reduce_partition: (reduce_partition[0], flatten(map(lambda reduce_input_group: REDUCE(*reduce_input_group), reduce_partition[1]))), reduce_partitions)\n",
    "\n",
    "    return list(reduce_outputs)\n",
    "\n",
    "partitioned_output = MapReduceDistributed(INPUTFORMAT, MAP, REDUCE, COMBINER=REDUCE, PARTITIONER=PARTITIONER)\n",
    "partitioned_output = [(partition_id, list(partition)) for (partition_id, partition) in partitioned_output]\n",
    "print(\"Partitioned Output:\", partitioned_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy65YJTH99iT"
   },
   "source": [
    "# Упражнения\n",
    "Упражнения взяты из Rajaraman A., Ullman J. D. Mining of massive datasets. – Cambridge University Press, 2011.\n",
    "\n",
    "Для выполнения заданий переопределите функции RECORDREADER, MAP, REDUCE. \n",
    "\n",
    "Для модели распределённой системы может потребоваться переопределение функций PARTITION и COMBINER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfvAeZm3S8S8"
   },
   "source": [
    "### Максимальное значение ряда\n",
    "\n",
    "Разработайте MapReduce алгоритм, который находит максимальное число входного списка чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3GRA1JR-Tkbg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение: 0.9866881591941783\n",
      "[0.10441231 0.30589425 0.52364608 0.38852965 0.97687711 0.91584597\n",
      " 0.69201001 0.35109474 0.22584654 0.6830471  0.40947403 0.5452787\n",
      " 0.48745994 0.22286615 0.01792167 0.86870867 0.83879789 0.23507257\n",
      " 0.32482869 0.26457599 0.32001808 0.85147695 0.04275044 0.83394052\n",
      " 0.18517443 0.09596378 0.11159597 0.98668816 0.32371741 0.55871168]\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import numpy as np\n",
    "\n",
    "d1 = np.random.rand(30)\n",
    "\n",
    "def RECORDREADER():\n",
    "    for value in d1:\n",
    "        yield (value, None)\n",
    "\n",
    "def MAP(value: float, _):\n",
    "    yield (value, None)\n",
    "\n",
    "def REDUCE(key: float, values: Iterator[None]):\n",
    "    yield key\n",
    "\n",
    "def flatten(nested_iterable):\n",
    "    for iterable in nested_iterable:\n",
    "        for element in iterable:\n",
    "            yield element\n",
    "\n",
    "def groupbykey(iterable):\n",
    "    t = {}\n",
    "    for (k2, v2) in iterable:\n",
    "        t[k2] = t.get(k2, []) + [v2]\n",
    "    return t.items()\n",
    "\n",
    "def MapReduce(RECORDREADER, MAP, REDUCE):\n",
    "    return flatten(map(lambda x: REDUCE(*x), groupbykey(flatten(map(lambda x: MAP(*x), RECORDREADER())))))\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "max_value = max(output)\n",
    "print(f\"Максимальное значение: {max_value}\")\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k86bXnqZTk-U"
   },
   "source": [
    "### Арифметическое среднее\n",
    "\n",
    "Разработайте MapReduce алгоритм, который находит арифметическое среднее.\n",
    "\n",
    "$$\\overline{X} = \\frac{1}{n}\\sum_{i=0}^{n} x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MPoY5pkfUNZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Арифметическое среднее: 0.10441231107112858\n"
     ]
    }
   ],
   "source": [
    "def REDUCE(key: float, values: Iterator[None]):\n",
    "    total_sum = 0\n",
    "    count = 0\n",
    "    for v in values:\n",
    "        total_sum += key  # суммируем все числа\n",
    "        count += 1  # увеличиваем количество\n",
    "    yield (total_sum, count)\n",
    "    \n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "total_sum, count = next(output)\n",
    "average = total_sum / count\n",
    "print(f\"Арифметическое среднее: {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SgEjCZyGnu6"
   },
   "source": [
    "### Drop duplicates (set construction, unique elements, distinct)\n",
    "\n",
    "Реализуйте распределённую операцию исключения дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "okjbyApjGhMt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальные значения: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "d1 = [1, 1, 2, 2, 3, 4, 5]\n",
    "\n",
    "def REDUCE(key: float, values: Iterator[None]):\n",
    "    yield (key)\n",
    "    \n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "unique_values = list(output)\n",
    "print(f\"Уникальные значения: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7sRGoTXuJze"
   },
   "source": [
    "## Операторы реляционной алгебры\n",
    "### Selection (Выборка)\n",
    "\n",
    "**The Map Function**: Для  каждого кортежа $t \\in R$ вычисляется истинность предиката $C$. В случае истины создаётся пара ключ-значение $(t, t)$. В паре ключ и значение одинаковы, равны $t$.\n",
    "\n",
    "**The Reduce Function:** Роль функции Reduce выполняет функция идентичности, которая возвращает то же значение, что получила на вход.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4nKIKe59uIfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранные значения: [np.float64(0.76), np.float64(0.89), np.float64(0.67), np.float64(0.56), np.float64(0.98), np.float64(0.78), np.float64(0.91), np.float64(0.64), np.float64(0.51), np.float64(0.73), np.float64(0.82), np.float64(0.57), np.float64(0.61), np.float64(0.95)]\n"
     ]
    }
   ],
   "source": [
    "d1 = np.array([0.23, 0.76, 0.89, 0.12, 0.45, 0.67, 0.34, 0.56, 0.98, 0.34,\n",
    "              0.23, 0.67, 0.78, 0.45, 0.56, 0.89, 0.76, 0.91, 0.43, 0.89,\n",
    "              0.25, 0.64, 0.51, 0.73, 0.82, 0.39, 0.57, 0.61, 0.95, 0.33])\n",
    "\n",
    "def predicate(value: float):\n",
    "    return value > 0.5  # Предикат: выбираем только те элементы, которые больше 0.5\n",
    "\n",
    "def RECORDREADER():\n",
    "    for value in d1:\n",
    "        if predicate(value):  # Применяем предикат перед маппингом\n",
    "            yield (value, None)\n",
    "\n",
    "def REDUCE(key: float, values: Iterator[None]):\n",
    "    yield key\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "selected_values = list(output)\n",
    "print(f\"Выбранные значения: {selected_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w27Ca-_Ku85V"
   },
   "source": [
    "### Projection (Проекция)\n",
    "\n",
    "Проекция на множество атрибутов $S$.\n",
    "\n",
    "**The Map Function:** Для каждого кортежа $t \\in R$ создайте кортеж $t′$, исключая  из $t$ те значения, атрибуты которых не принадлежат  $S$. Верните пару $(t′, t′)$.\n",
    "\n",
    "**The Reduce Function:** Для каждого ключа $t′$, созданного любой Map задачей, вы получаете одну или несколько пар $(t′, t′)$. Reduce функция преобразует $(t′, [t′, t′, . . . , t′])$ в $(t′, t′)$, так, что для ключа $t′$ возвращается одна пара  $(t′, t′)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BEvuY4GqvhS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проекция на атрибуты [1, 2]: [('Alice', 25), ('Bob', 30), ('Charlie', 22), ('David', 35), ('Eve', 28), ('Frank', 40)]\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "d1 = [\n",
    "    (1, \"Alice\", 25, \"Engineer\"),\n",
    "    (2, \"Bob\", 30, \"Doctor\"),\n",
    "    (3, \"Charlie\", 22, \"Artist\"),\n",
    "    (4, \"David\", 35, \"Lawyer\"),\n",
    "    (5, \"Eve\", 28, \"Scientist\"),\n",
    "    (6, \"Frank\", 40, \"Engineer\"),\n",
    "]\n",
    "\n",
    "# Атрибуты, которые оставляем (например, только имя и возраст)\n",
    "S = [1, 2]  # Индексы атрибутов в кортежах\n",
    "\n",
    "def RECORDREADER():\n",
    "    for record in d1:\n",
    "        yield tuple(record[i] for i in S), None  # Оставляем нужные атрибуты\n",
    "\n",
    "# Функция REDUCE: убирает дубликаты\n",
    "def REDUCE(key: Tuple, values: Iterator[None]):\n",
    "    yield key  # Возвращаем только один экземпляр t'\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "print(f\"Проекция на атрибуты {S}: {list(output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gau6lKXvn2R"
   },
   "source": [
    "### Union (Объединение)\n",
    "\n",
    "**The Map Function:** Превратите каждый входной кортеж $t$ в пару ключ-значение $(t, t)$.\n",
    "\n",
    "**The Reduce Function:** С каждым ключом $t$ будет ассоциировано одно или два значений. В обоих случаях создайте $(t, t)$ в качестве выходного значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Sns7a5agv3nw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объединение множеств R и S: [(1, 'Alice'), (2, 'Bob'), (3, 'Charlie'), (4, 'David'), (5, 'Eve')]\n"
     ]
    }
   ],
   "source": [
    "R = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "S = [(3, \"Charlie\"), (4, \"David\"), (5, \"Eve\")]\n",
    "\n",
    "# Объединённый источник данных\n",
    "def RECORDREADER():\n",
    "    for record in R + S:  # Объединяем R и S\n",
    "        yield (record, None)\n",
    "\n",
    "# Функция REDUCE: просто возвращает ключ, так как он уже является кортежем (t, t)\n",
    "def REDUCE(key: Tuple, values: Iterator[Tuple]):\n",
    "    yield key\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "union_result = list(output)\n",
    "print(f\"Объединение множеств R и S: {union_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ8TuEbjv4J8"
   },
   "source": [
    "### Intersection (Пересечение)\n",
    "\n",
    "**The Map Function:** Превратите каждый кортеж $t$ в пары ключ-значение $(t, t)$.\n",
    "\n",
    "**The Reduce Function:** Если для ключа $t$ есть список из двух элементов $[t, t]$ $-$ создайте пару $(t, t)$. Иначе, ничего не создавайте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XKlBZh4IwERR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пересечение множеств R и S: [(3, 'Charlie')]\n"
     ]
    }
   ],
   "source": [
    "# Функция REDUCE: оставляет только те ключи, которые встречаются дважды\n",
    "def REDUCE(key: Tuple, values: Iterator[Tuple]):\n",
    "    values_list = list(values)  # Преобразуем в список\n",
    "    if len(values_list) > 1:  # Если есть два вхождения (из R и S)\n",
    "        yield key  # Это пересечение\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "intersection_result = list(output)\n",
    "print(f\"Пересечение множеств R и S: {intersection_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVOpqoY3wE5k"
   },
   "source": [
    "### Difference (Разница)\n",
    "\n",
    "**The Map Function:** Для кортежа $t \\in R$, создайте пару $(t, R)$, и для кортежа $t \\in S$, создайте пару $(t, S)$. Задумка заключается в том, чтобы значение пары было именем отношения $R$ or $S$, которому принадлежит кортеж (а лучше, единичный бит, по которому можно два отношения различить $R$ or $S$), а не весь набор атрибутов отношения.\n",
    "\n",
    "**The Reduce Function:** Для каждого ключа $t$, если соответствующее значение является списком $[R]$, создайте пару $(t, t)$. В иных случаях не предпринимайте действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QE_AC09lwZIZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разность множеств R - S: [((1, 'Alice'), (1, 'Alice')), ((2, 'Bob'), (2, 'Bob'))]\n"
     ]
    }
   ],
   "source": [
    "def RECORDREADER():\n",
    "    for number in R:\n",
    "        yield (number, \"R\") \n",
    "    for number in S:\n",
    "        yield (number, \"S\") \n",
    "        \n",
    "def MAP(value, key):\n",
    "    yield (value, key)\n",
    "\n",
    "def REDUCE(key, values):\n",
    "    if values == [\"R\"]:\n",
    "        yield (key, key)\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "difference_result = list(output)\n",
    "print(f\"Разность множеств R - S: {difference_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8I58V2VwhSm"
   },
   "source": [
    "### Natural Join\n",
    "\n",
    "**The Map Function:** Для каждого кортежа $(a, b)$ отношения $R$, создайте пару $(b,(R, a))$. Для каждого кортежа $(b, c)$ отношения $S$, создайте пару $(b,(S, c))$.\n",
    "\n",
    "**The Reduce Function:** Каждый ключ $b$ будет асоциирован со списком пар, которые принимают форму либо $(R, a)$, либо $(S, c)$. Создайте все пары, одни, состоящие из  первого компонента $R$, а другие, из первого компонента $S$, то есть $(R, a)$ и $(S, c)$. На выходе вы получаете последовательность пар ключ-значение из списков ключей и значений. Ключ не нужен. Каждое значение, это тройка $(a, b, c)$ такая, что $(R, a)$ и $(S, c)$ это принадлежат входному списку значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yHiuuTctw86I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат Natural Join: [(2, 'Bob', 30), (3, 'Charlie', 10)]\n"
     ]
    }
   ],
   "source": [
    "R = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "S = [(\"Charlie\", 10), (\"David\", 20), (\"Bob\", 30)]\n",
    "\n",
    "def RECORDREADER():\n",
    "    for (a, b) in R:\n",
    "        yield (b, None)\n",
    "    for (b, c) in S:\n",
    "        yield (b, None)\n",
    "\n",
    "def MAP(value: float, _):\n",
    "    yield (value, None)\n",
    "\n",
    "def REDUCE(key, values):\n",
    "    r_vals = [a for (a, b) in R if b == key]\n",
    "    s_vals = [c for (b, c) in S if b == key]\n",
    "    \n",
    "    for a in r_vals:\n",
    "        for c in s_vals:\n",
    "            yield (a, key, c)\n",
    "\n",
    "\n",
    "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
    "natural_join_result = list(output)\n",
    "print(\"Результат Natural Join:\", natural_join_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYdlr0YUxE27"
   },
   "source": [
    "### Grouping and Aggregation (Группировка и аггрегация)\n",
    "\n",
    "**The Map Function:** Для каждого кортежа $(a, b, c$) создайте пару $(a, b)$.\n",
    "\n",
    "**The Reduce Function:** Ключ представляет ту или иную группу. Примение аггрегирующую операцию $\\theta$ к списку значений $[b1, b2, . . . , bn]$ ассоциированных с ключом $a$. Возвращайте в выходной поток $(a, x)$, где $x$ результат применения  $\\theta$ к списку. Например, если $\\theta$ это $SUM$, тогда $x = b1 + b2 + · · · + bn$, а если $\\theta$ is $MAX$, тогда $x$ это максимальное из значений $b1, b2, . . . , bn$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MLPckfEGxico"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат агрегации (SUM): [(1, 30), (2, 70)]\n",
      "Результат агрегации (MAX): [(1, 20), (2, 40)]\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 10, \"A\"), (1, 20, \"B\"), (2, 30, \"C\"), (2, 40, \"D\")]\n",
    "\n",
    "def MAP(key,value):\n",
    "    yield (key, value)\n",
    "\n",
    "def RECORDREADER():\n",
    "    for item in data:\n",
    "        yield (item[0], item[1])\n",
    "\n",
    "def REDUCE(key: float, values: Iterator[None], operation=\"SUM\"):\n",
    "    if operation == \"SUM\":\n",
    "        yield (key, sum(values)) \n",
    "    if operation == \"MAX\":\n",
    "        yield (key, max(values)) \n",
    "\n",
    "output_sum = MapReduce(RECORDREADER, MAP, REDUCE) \n",
    "aggregation_result_sum = list(output_sum)\n",
    "print(\"Результат агрегации (SUM):\", aggregation_result_sum)\n",
    "\n",
    "output_max = MapReduce(RECORDREADER, MAP, lambda key, values: REDUCE(key, values, operation=\"MAX\"))\n",
    "aggregation_result_max = list(output_max)\n",
    "print(\"Результат агрегации (MAX):\", aggregation_result_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление TF-IDF (Term Frequency – Inverse Document Fraquency)\n",
    "\n",
    "Реализуется в три этапа:\n",
    "\n",
    "**Этап 1:** Частота слова в документе\n",
    "\n",
    "**Этап 2:** Количество документов, в которых встречается слово\n",
    "\n",
    "**Этап 3:** Расчёт TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = [\n",
    "    \"\"\"\n",
    "    Streaming data is the data from sensors as well as other real-time surveillance systems. Distributed stream processing systems are the software that manages such data. Such frameworks have to deliver outcomes on the go instantly. They are susceptible to delay and malfunction or system failures. The system must be tolerant of faults and always accessible. Many variables, such as improved network arrival rates, node failures, and so on, disrupt the system's reliability. Some operators need to be relocated online from one physical resource to another to manage or reimburse a slow or failing node. In this study, we propose a co-location based systematic migration heuristic for live operator migration between physical resources using a migration map revised with costs for each migration. The suggested method evaluates continuous operator performance patterns and makes online scheduling decisions based on the same. The decisions include migrating operators during a node failure or straggling.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Distributed stream processing engines are designed with a focus on scalability to process big data volumes in a continuous manner. We present the Theodolite method for benchmarking the scalability of distributed stream processing engines. Core of this method is the definition of use cases that microservices implementing stream processing have to fulfill. For each use case, our method identifies relevant workload dimensions that might affect the scalability of a use case. We propose to design one benchmark per use case and relevant workload dimension. We present a general benchmarking framework, which can be applied to execute the individual benchmarks for a given use case and workload dimension. Our framework executes an implementation of the use case's dataflow architecture for different workloads of the given dimension and various numbers of processing instances. This way, it identifies how resources demand evolves with increasing workloads. Within the scope of this paper, we present 4 identified use cases, derived from processing Industrial Internet of Things data, and 7 corresponding workload dimensions. We provide implementations of 4 benchmarks with Kafka Streams and Apache Flink as well as an implementation of our benchmarking framework to execute scalability benchmarks in cloud environments. We use both for evaluating the Theodolite method and for benchmarking Kafka Streams' and Flink's scalability for different deployment options.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Batch and stream processing are separately and efficiently applied in many applications. However, some newer data-driven applications such as the Internet of Things and cloud computing call for hybrid processing approaches in order to handle the speed and accuracy required for processing such complex data. In this paper, we propose a Hybrid Distributed Batch-Stream (HDBS) architecture for anomaly detection in real-time data. The hybrid architecture, while benefiting from the accuracy provided by batch processing, also enjoys the speed and real-time features of stream processing. In the proposed architecture, our focus is on the algorithmic aspects of hybrid processing including the interaction models between batch and stream processing units, the characteristics of batch and stream machine learning algorithms and the principles of merging the results of different processing units. The driving idea of such combination is that the results of batch and stream processing units are complementary with each other, as one of them constructs accurate models based on previous data, and the other one is capable of processing new stream data in real-time. Furthermore, we propose a generalized version of the HDBS with respect to its algorithms and communication policy levels. In the generalized HDBS architecture, we address the various aspects of the interaction between the batch and stream processing units, and the merging operations to produce the final results. the evaluations of the proposed architecture using various criteria (accuracy, space complexity, and time complexity) demonstrate that the accuracy of the proposed method is higher than the accuracy of the batch processing methods, its time complexity is also similar to one of the stream processing methods and much less than the batch processing methods, which makes our proposed architecture an efficient and practical solution for real-time anomaly detection.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    There have been increasing demands for real time processing of the ever-growing data. In order to meet this requirement and ensure the reliable processing of streaming data, a variety of distributed stream processing architectures and platforms have been developed, which handles the fundamental task of allocating processing tasks to the currently available physical resources and routing streaming data between these resources. However, many stream processing systems lack an intelligent scheduling mechanism, in which their default schedulers allocate tasks without taking resource demands and availability, or the transfer latency between resources into consideration. Besides, stream processing has a strict request for latency. Thus it is important to give latency guarantee for distributed stream processing. In this paper, we propose two new algorithms for stream processing with latency guarantee, both the algorithms consider transfer latency and resource demand in task allocation. Both algorithms can guarantee latency constraints. Algorithm AHA reduces more than 21.3% and 58.9% resources compared with the greedy and the round-robin algorithms, and algorithm PHA further improves the resource utilization to 32.1% and 73.2%.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    In the era of Big Data, typical architecture of distributed real-time stream processing systems is the combination of Flume, Kafka, and Storm. As a kind of distributed message system, Kafka has the characteristics of horizontal scalability and high throughput, which is manly deployed in many areas in order to address the problem of speed mismatch between message producers and consumers. When using Kafka, we need to quickly receive data sent by producers. In addition, we need to send data to consumers quickly. Therefore, the performance of Kafka is of critical importance to the performance of the whole stream processing system. In this paper, we propose the improved design of real-time stream processing systems, and focus on improving the Kafka’s data loading process. We use Kafka cat to transfer data from the source to Kafka topic directly, which can reduce the network transmission. We also utilize the memory file system to accelerate the process of data loading, which can address the bottleneck and performance problems caused by disk I/O. Extensive experiments are conducted to evaluate the performance, which show the superiority of our improved design.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    In this paper, nearly 40 commonly used deep neural network(DNN) models are selected, and their cross-platform and cross-inference frameworks are deeply analysed. The main metrics of accuracy, the total number of model parameters, the computational complexity, the accuracy density, the inference time, the memory consumption and other related parameters are used to measure their performance. The heterogeneous computing experiment is implemented on both the Google Colab cloud computing platform and the Jetson Nano embedded edge computing platform. The obtained performance is compared with that of two previous computing platforms: a workstation equipped with an NVIDIA Titan X Pascal and an embedded system based on an NVIDIA Jetson TX1 board. In addition, on the Jetson Nano embedded edge computing platform, different inference frameworks are investigated to evaluate the inference efficiency of the DNN models. Regression models are established to characterize the variation in the computing performance of different DNN classification algorithms so that the inference results of unknown models can be estimated. ANOVA methods are proposed to quantify the differences between models. The experimental results have important guiding significance for the better selection, deployment and application of DNN models in practice. Codes are available at this https URL https://github.com/Foreverzfy/Model-Test.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Unmanned Aerial Vehicles (UAVs), which can operate autonomously in dynamic and complex environments, are becoming increasingly common. Deep learning techniques for motion control have recently taken a major qualitative step since vision-based inference tasks can be executed directly on edge. The goal is to fully integrate the machine learning (ML) element into small UAVs. However, given the limited payload capacity and energy available on small UAVs, integrating computing resources sufficient to host ML and vehicle control functions is still challenging. This paper presents a modular and generic system that can control the UAV by evaluating vision-based ML tasks directly inside the resource-constrained UAV. Two different vision-based navigation configurations were tested and demonstrated. The first configuration implements an autonomous landing site detection system, tested with two models based on LeNet-5 and MobileNetV2, respectively. This allows the UAV to change its planned path accordingly and approach the target to land. Moreover, a model for people detection based on a custom MobileNetV2 network was evaluated in the second configuration. Finally, the execution time and power consumption were measured and compared with a cloud computing approach. The results show the ability of the developed system to dynamically react to the environment to provide the necessary maneuver after detecting the target exploiting only the constrained computational resources of the UAV controller. Furthermore, we demonstrated that moving to the edge, instead of using cloud computing inference, decreases the energy requirement of the system without reducing the quality of service.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    With the continuous development of Internet of Things (IoT) and the overwhelming explosion of Big Data, edge computing serves as an efficient computing mode for time stringent data processing, which can bypass the constraints of network bandwidth and delay, and has been one of the foundation of interconnected applications. Although edge computing has gradually become one of bridges between cloud computing centers and mobile terminals, the literature still lacks a thorough review on the recent advances in edge computing platforms. In this paper, we firstly introduce the definition of edge computing and advantages of edge computing platform. And then, we summarize the key technologies of constructing an edge computing platform, and propose a general framework for edge computing platform. The role of distributed storage management systems in building edge computing platform is elaborated in detail. Furthermore, we give some applications to illustrate how to use third-party edge computing platforms to build specific applications. Finally, we briefly outline current open issues of edge computing platform based on our literature survey.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "cleaned_docs = [clean_text(doc) for doc in annotations]\n",
    "N = len(cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "def tf_record_reader():\n",
    "    for doc_id, text in enumerate(cleaned_docs):\n",
    "        yield (doc_id, text)\n",
    "\n",
    "def tf_map(doc_id: int, text: str):\n",
    "    words = text.split()\n",
    "    total = len(words)\n",
    "    for word in words:\n",
    "        yield ((doc_id, word), 1/total)\n",
    "\n",
    "def tf_reducer(key, values):\n",
    "    yield (key, sum(values))\n",
    "\n",
    "tf_results = list(MapReduce(tf_record_reader, tf_map, tf_reducer))\n",
    "tf_dict = {(doc, word): val for (doc, word), val in tf_results}\n",
    "\n",
    "# IDF \n",
    "def idf_record_reader():\n",
    "    for doc_id, text in enumerate(cleaned_docs):\n",
    "        yield (doc_id, text)\n",
    "\n",
    "def idf_map(doc_id: int, text: str):\n",
    "    for word in set(text.split()):\n",
    "        yield (word, doc_id)\n",
    "\n",
    "def idf_reducer(word: str, doc_ids: Iterator[int]):\n",
    "    yield (word, math.log(N / len(set(doc_ids))))\n",
    "\n",
    "idf_results = dict(MapReduce(idf_record_reader, idf_map, idf_reducer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Документ 1\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+-----------+--------+\n",
      "| Слово     |     TF |\n",
      "+===========+========+\n",
      "| the       | 0.0533 |\n",
      "+-----------+--------+\n",
      "| to        | 0.0333 |\n",
      "+-----------+--------+\n",
      "| a         | 0.0267 |\n",
      "+-----------+--------+\n",
      "| and       | 0.0267 |\n",
      "+-----------+--------+\n",
      "| migration | 0.0267 |\n",
      "+-----------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+------------+--------+\n",
      "| Слово      |    IDF |\n",
      "+============+========+\n",
      "| accessible | 2.0794 |\n",
      "+------------+--------+\n",
      "| always     | 2.0794 |\n",
      "+------------+--------+\n",
      "| another    | 2.0794 |\n",
      "+------------+--------+\n",
      "| arrival    | 2.0794 |\n",
      "+------------+--------+\n",
      "| colocation | 2.0794 |\n",
      "+------------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+-----------+----------+\n",
      "| Слово     |   TF-IDF |\n",
      "+===========+==========+\n",
      "| migration |   0.0555 |\n",
      "+-----------+----------+\n",
      "| node      |   0.0416 |\n",
      "+-----------+----------+\n",
      "| or        |   0.0370 |\n",
      "+-----------+----------+\n",
      "| decisions |   0.0277 |\n",
      "+-----------+----------+\n",
      "| failures  |   0.0277 |\n",
      "+-----------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 2\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+---------+--------+\n",
      "| Слово   |     TF |\n",
      "+=========+========+\n",
      "| of      | 0.0512 |\n",
      "+---------+--------+\n",
      "| the     | 0.0419 |\n",
      "+---------+--------+\n",
      "| use     | 0.0372 |\n",
      "+---------+--------+\n",
      "| and     | 0.0326 |\n",
      "+---------+--------+\n",
      "| for     | 0.0326 |\n",
      "+---------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+-----------+--------+\n",
      "| Слово     |    IDF |\n",
      "+===========+========+\n",
      "| 4         | 2.0794 |\n",
      "+-----------+--------+\n",
      "| 7         | 2.0794 |\n",
      "+-----------+--------+\n",
      "| affect    | 2.0794 |\n",
      "+-----------+--------+\n",
      "| apache    | 2.0794 |\n",
      "+-----------+--------+\n",
      "| benchmark | 2.0794 |\n",
      "+-----------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+--------------+----------+\n",
      "| Слово        |   TF-IDF |\n",
      "+==============+==========+\n",
      "| benchmarking |   0.0387 |\n",
      "+--------------+----------+\n",
      "| case         |   0.0387 |\n",
      "+--------------+----------+\n",
      "| workload     |   0.0387 |\n",
      "+--------------+----------+\n",
      "| use          |   0.0365 |\n",
      "+--------------+----------+\n",
      "| scalability  |   0.0322 |\n",
      "+--------------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 3\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+------------+--------+\n",
      "| Слово      |     TF |\n",
      "+============+========+\n",
      "| the        | 0.1007 |\n",
      "+------------+--------+\n",
      "| and        | 0.0556 |\n",
      "+------------+--------+\n",
      "| of         | 0.0556 |\n",
      "+------------+--------+\n",
      "| processing | 0.0486 |\n",
      "+------------+--------+\n",
      "| batch      | 0.0278 |\n",
      "+------------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+-------------+--------+\n",
      "| Слово       |    IDF |\n",
      "+=============+========+\n",
      "| accurate    | 2.0794 |\n",
      "+-------------+--------+\n",
      "| algorithmic | 2.0794 |\n",
      "+-------------+--------+\n",
      "| anomaly     | 2.0794 |\n",
      "+-------------+--------+\n",
      "| approaches  | 2.0794 |\n",
      "+-------------+--------+\n",
      "| aspects     | 2.0794 |\n",
      "+-------------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+----------+----------+\n",
      "| Слово    |   TF-IDF |\n",
      "+==========+==========+\n",
      "| batch    |   0.0578 |\n",
      "+----------+----------+\n",
      "| hybrid   |   0.0289 |\n",
      "+----------+----------+\n",
      "| units    |   0.0289 |\n",
      "+----------+----------+\n",
      "| accuracy |   0.0241 |\n",
      "+----------+----------+\n",
      "| hdbs     |   0.0217 |\n",
      "+----------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 4\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+------------+--------+\n",
      "| Слово      |     TF |\n",
      "+============+========+\n",
      "| and        | 0.0517 |\n",
      "+------------+--------+\n",
      "| the        | 0.0517 |\n",
      "+------------+--------+\n",
      "| processing | 0.0460 |\n",
      "+------------+--------+\n",
      "| latency    | 0.0345 |\n",
      "+------------+--------+\n",
      "| stream     | 0.0287 |\n",
      "+------------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+---------+--------+\n",
      "| Слово   |    IDF |\n",
      "+=========+========+\n",
      "| 213     | 2.0794 |\n",
      "+---------+--------+\n",
      "| 321     | 2.0794 |\n",
      "+---------+--------+\n",
      "| 589     | 2.0794 |\n",
      "+---------+--------+\n",
      "| 732     | 2.0794 |\n",
      "+---------+--------+\n",
      "| aha     | 2.0794 |\n",
      "+---------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+-----------+----------+\n",
      "| Слово     |   TF-IDF |\n",
      "+===========+==========+\n",
      "| latency   |   0.0717 |\n",
      "+-----------+----------+\n",
      "| guarantee |   0.0359 |\n",
      "+-----------+----------+\n",
      "| algorithm |   0.0239 |\n",
      "+-----------+----------+\n",
      "| demands   |   0.0239 |\n",
      "+-----------+----------+\n",
      "| resource  |   0.0239 |\n",
      "+-----------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 5\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+---------+--------+\n",
      "| Слово   |     TF |\n",
      "+=========+========+\n",
      "| the     | 0.0870 |\n",
      "+---------+--------+\n",
      "| of      | 0.0652 |\n",
      "+---------+--------+\n",
      "| to      | 0.0489 |\n",
      "+---------+--------+\n",
      "| data    | 0.0326 |\n",
      "+---------+--------+\n",
      "| kafka   | 0.0326 |\n",
      "+---------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+------------+--------+\n",
      "| Слово      |    IDF |\n",
      "+============+========+\n",
      "| accelerate | 2.0794 |\n",
      "+------------+--------+\n",
      "| areas      | 2.0794 |\n",
      "+------------+--------+\n",
      "| bottleneck | 2.0794 |\n",
      "+------------+--------+\n",
      "| cat        | 2.0794 |\n",
      "+------------+--------+\n",
      "| caused     | 2.0794 |\n",
      "+------------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+-----------+----------+\n",
      "| Слово     |   TF-IDF |\n",
      "+===========+==========+\n",
      "| kafka     |   0.0452 |\n",
      "+-----------+----------+\n",
      "| consumers |   0.0226 |\n",
      "+-----------+----------+\n",
      "| loading   |   0.0226 |\n",
      "+-----------+----------+\n",
      "| message   |   0.0226 |\n",
      "+-----------+----------+\n",
      "| producers |   0.0226 |\n",
      "+-----------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 6\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+-----------+--------+\n",
      "| Слово     |     TF |\n",
      "+===========+========+\n",
      "| the       | 0.0960 |\n",
      "+-----------+--------+\n",
      "| are       | 0.0354 |\n",
      "+-----------+--------+\n",
      "| of        | 0.0354 |\n",
      "+-----------+--------+\n",
      "| and       | 0.0303 |\n",
      "+-----------+--------+\n",
      "| computing | 0.0303 |\n",
      "+-----------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+-------------+--------+\n",
      "| Слово       |    IDF |\n",
      "+=============+========+\n",
      "| 40          | 2.0794 |\n",
      "+-------------+--------+\n",
      "| analysed    | 2.0794 |\n",
      "+-------------+--------+\n",
      "| anova       | 2.0794 |\n",
      "+-------------+--------+\n",
      "| application | 2.0794 |\n",
      "+-------------+--------+\n",
      "| at          | 2.0794 |\n",
      "+-------------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+-----------+----------+\n",
      "| Слово     |   TF-IDF |\n",
      "+===========+==========+\n",
      "| dnn       |   0.0315 |\n",
      "+-----------+----------+\n",
      "| embedded  |   0.0315 |\n",
      "+-----------+----------+\n",
      "| jetson    |   0.0315 |\n",
      "+-----------+----------+\n",
      "| models    |   0.0297 |\n",
      "+-----------+----------+\n",
      "| inference |   0.0280 |\n",
      "+-----------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 7\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+---------+--------+\n",
      "| Слово   |     TF |\n",
      "+=========+========+\n",
      "| the     | 0.0905 |\n",
      "+---------+--------+\n",
      "| and     | 0.0370 |\n",
      "+---------+--------+\n",
      "| to      | 0.0329 |\n",
      "+---------+--------+\n",
      "| a       | 0.0206 |\n",
      "+---------+--------+\n",
      "| of      | 0.0206 |\n",
      "+---------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+-------------+--------+\n",
      "| Слово       |    IDF |\n",
      "+=============+========+\n",
      "| ability     | 2.0794 |\n",
      "+-------------+--------+\n",
      "| accordingly | 2.0794 |\n",
      "+-------------+--------+\n",
      "| aerial      | 2.0794 |\n",
      "+-------------+--------+\n",
      "| after       | 2.0794 |\n",
      "+-------------+--------+\n",
      "| allows      | 2.0794 |\n",
      "+-------------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+-------------+----------+\n",
      "| Слово       |   TF-IDF |\n",
      "+=============+==========+\n",
      "| uav         |   0.0342 |\n",
      "+-------------+----------+\n",
      "| control     |   0.0257 |\n",
      "+-------------+----------+\n",
      "| ml          |   0.0257 |\n",
      "+-------------+----------+\n",
      "| uavs        |   0.0257 |\n",
      "+-------------+----------+\n",
      "| visionbased |   0.0257 |\n",
      "+-------------+----------+\n",
      "\n",
      "========================================\n",
      "Документ 8\n",
      "========================================\n",
      "\n",
      "Term Frequency (TF):\n",
      "+-----------+--------+\n",
      "| Слово     |     TF |\n",
      "+===========+========+\n",
      "| computing | 0.0710 |\n",
      "+-----------+--------+\n",
      "| of        | 0.0710 |\n",
      "+-----------+--------+\n",
      "| edge      | 0.0592 |\n",
      "+-----------+--------+\n",
      "| the       | 0.0533 |\n",
      "+-----------+--------+\n",
      "| and       | 0.0414 |\n",
      "+-----------+--------+\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "+------------+--------+\n",
      "| Слово      |    IDF |\n",
      "+============+========+\n",
      "| advances   | 2.0794 |\n",
      "+------------+--------+\n",
      "| advantages | 2.0794 |\n",
      "+------------+--------+\n",
      "| although   | 2.0794 |\n",
      "+------------+--------+\n",
      "| bandwidth  | 2.0794 |\n",
      "+------------+--------+\n",
      "| become     | 2.0794 |\n",
      "+------------+--------+\n",
      "\n",
      "TF-IDF:\n",
      "+--------------+----------+\n",
      "| Слово        |   TF-IDF |\n",
      "+==============+==========+\n",
      "| edge         |   0.0580 |\n",
      "+--------------+----------+\n",
      "| computing    |   0.0492 |\n",
      "+--------------+----------+\n",
      "| platform     |   0.0410 |\n",
      "+--------------+----------+\n",
      "| applications |   0.0246 |\n",
      "+--------------+----------+\n",
      "| literature   |   0.0246 |\n",
      "+--------------+----------+\n",
      "\n",
      "========================================\n",
      "Глобальный топ TF-IDF\n",
      "========================================\n",
      "+-----------+----------+\n",
      "| Слово     |   TF-IDF |\n",
      "+===========+==========+\n",
      "| latency   |   0.0717 |\n",
      "+-----------+----------+\n",
      "| edge      |   0.0580 |\n",
      "+-----------+----------+\n",
      "| batch     |   0.0578 |\n",
      "+-----------+----------+\n",
      "| migration |   0.0555 |\n",
      "+-----------+----------+\n",
      "| computing |   0.0492 |\n",
      "+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "def print_document_stats(doc_id):\n",
    "    print(f\"\\n{'='*40}\\nДокумент {doc_id+1}\\n{'='*40}\")\n",
    "    \n",
    "#TF\n",
    "    tf_items = [(word, val) for (d, word), val in tf_dict.items() if d == doc_id]\n",
    "    tf_top = sorted(tf_items, key=lambda x: (-x[1], x[0]))[:5]\n",
    "    print(\"\\nTerm Frequency (TF):\")\n",
    "    print(tabulate(tf_top, headers=[\"Слово\", \"TF\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "    \n",
    "#IDF\n",
    "    doc_words = {word for (d, word) in tf_dict.keys() if d == doc_id}\n",
    "    idf_items = [(word, idf_results[word]) for word in doc_words if word in idf_results]\n",
    "    idf_top = sorted(idf_items, key=lambda x: (-x[1], x[0]))[:5]\n",
    "    print(\"\\nInverse Document Frequency (IDF):\")\n",
    "    print(tabulate(idf_top, headers=[\"Слово\", \"IDF\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "    \n",
    "#TF-IDF\n",
    "    tfidf_items = [(word, score) for word, score in tfidf[doc_id].items()]\n",
    "    tfidf_top = sorted(tfidf_items, key=lambda x: (-x[1], x[0]))[:5]\n",
    "    print(\"\\nTF-IDF:\")\n",
    "    print(tabulate(tfidf_top, headers=[\"Слово\", \"TF-IDF\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "\n",
    "#Вывод статистики для каждого документа\n",
    "for doc_id in range(N):\n",
    "    print_document_stats(doc_id)\n",
    "\n",
    "#Глобальный TF-IDF\n",
    "print(\"\\n\"+ \"=\"*40 + \"\\nГлобальный топ TF-IDF\\n\" + \"=\"*40)\n",
    "all_tfidf = [(word, score) for doc in tfidf.values() for word, score in doc.items()]\n",
    "global_tfidf = sorted(all_tfidf, key=lambda x: (-x[1], x[0]))[:5]\n",
    "print(tabulate(global_tfidf, headers=[\"Слово\", \"TF-IDF\"], tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
